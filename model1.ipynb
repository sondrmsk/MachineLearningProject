{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FÃ¸rste Utkast Autogulon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autogluon import\n",
    "\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Loading and Preparing the Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"lakkenNoteBook.ipynb\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Preprocessed training and test data imported from lakkenNoteBook\n",
    "\n",
    "train_data = ais_train_enriched\n",
    "test_data = ais_test_enriched\n",
    "\n",
    "# Display first few rows of the data to verify loading\n",
    "# train_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ais_train_enriched.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Splitting training data into training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(to_model_df, split_date=\"2024-03-15\", holdout_frac=0.2, version=\"date_split\", seed=42):\n",
    "    \"\"\"\n",
    "    Custom train-validation split for vessel trajectory prediction based on time.\n",
    "    \n",
    "    Params:\n",
    "    - to_model_df: The dataframe containing all features and target columns.\n",
    "    - split_date: The cutoff date to separate training and validation sets.\n",
    "    - holdout_frac: Fraction of the validation data to hold out for testing after training.\n",
    "    - version: The type of splitting to perform. Options: \"date_split\", \"random_sampled\", etc.\n",
    "    - seed: Random seed for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - training_data: DataFrame of training data.\n",
    "    - validation_data: DataFrame of validation data.\n",
    "    - holdout: DataFrame for holdout (testing).\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Splitting dataset based on {version}...\")\n",
    "\n",
    "    # Sort the dataframe by 'time' to respect the temporal order\n",
    "    to_model_df = to_model_df.sort_values(by=\"time\").reset_index(drop=True)\n",
    "    \n",
    "    if version == \"date_split\":\n",
    "        # Split into training and validation based on the split_date\n",
    "        training_data = to_model_df[to_model_df[\"time\"] < split_date]\n",
    "        validation_data = to_model_df[to_model_df[\"time\"] >= split_date]\n",
    "    elif version == \"random_sampled\":\n",
    "        # Randomly split a portion of the data for validation\n",
    "        training_data, validation_data = train_test_split(to_model_df, test_size=0.2, random_state=seed)\n",
    "    else:\n",
    "        print(\"Invalid splitting type, defaulting to date-based split\")\n",
    "        training_data = to_model_df[to_model_df[\"time\"] < split_date]\n",
    "        validation_data = to_model_df[to_model_df[\"time\"] >= split_date]\n",
    "\n",
    "    print(f\"Training data from {training_data['time'].min()} to {training_data['time'].max()}\")\n",
    "    print(f\"Validation data from {validation_data['time'].min()} to {validation_data['time'].max()}\")\n",
    "\n",
    "    # Create holdout set from the validation data, with a random sample from validation\n",
    "    holdout = validation_data.sample(frac=holdout_frac, random_state=seed)\n",
    "    validation_data = validation_data.drop(holdout.index)\n",
    "\n",
    "    return training_data, validation_data, holdout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor, TabularDataset\n",
    "\n",
    "# Split the data\n",
    "split_date = \"2024-03-15\"  # The date chosen for splitting\n",
    "train_data, val_data, holdout_data = train_val_split(ais_train_enriched, split_date=split_date, holdout_frac=0.2)\n",
    "\n",
    "# Prepare the data for AutoGluon (remove columns that should not be used for training)\n",
    "drop_columns = ['time', 'vesselId']  # Adjust this list based on features to exclude\n",
    "train_data = TabularDataset(train_data.drop(columns=drop_columns))\n",
    "val_data = TabularDataset(val_data.drop(columns=drop_columns))\n",
    "holdout_data = TabularDataset(holdout_data.drop(columns=drop_columns))\n",
    "\n",
    "# Define the targets separately\n",
    "latitude_target = 'latitude'\n",
    "longitude_target = 'longitude'\n",
    "\n",
    "# Train the first model for latitude prediction\n",
    "latitude_predictor = TabularPredictor(\n",
    "    problem_type=\"regression\",\n",
    "    label=latitude_target,\n",
    "    eval_metric=\"mean_absolute_error\",  # You can adjust this based on your preference\n",
    "    path=\"autogluon_latitude_model\",\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"Training the model for latitude prediction...\")\n",
    "latitude_predictor.fit(\n",
    "    train_data,\n",
    "    tuning_data=val_data,  # Use validation data for tuning\n",
    "    time_limit=60 * 60,  # Time limit of 1 hour\n",
    "    presets=\"best_quality\",  # Prioritize best quality\n",
    "    excluded_model_types=[\"KNN\"],  # Exclude unsuitable models for trajectory prediction\n",
    ")\n",
    "print(\"Latitude model training complete!\")\n",
    "\n",
    "# Train the second model for longitude prediction\n",
    "longitude_predictor = TabularPredictor(\n",
    "    problem_type=\"regression\",\n",
    "    label=longitude_target,\n",
    "    eval_metric=\"mean_absolute_error\",  # You can adjust this based on your preference\n",
    "    path=\"autogluon_longitude_model\",\n",
    "    verbosity=2\n",
    ")\n",
    "\n",
    "print(\"Training the model for longitude prediction...\")\n",
    "longitude_predictor.fit(\n",
    "    train_data,\n",
    "    tuning_data=val_data,  # Use validation data for tuning\n",
    "    time_limit=60 * 60,  # Time limit of 1 hour\n",
    "    presets=\"best_quality\",  # Prioritize best quality\n",
    "    excluded_model_types=[\"KNN\"],  # Exclude unsuitable models for trajectory prediction\n",
    ")\n",
    "print(\"Longitude model training complete!\")\n",
    "\n",
    "# Once both models are trained, you can evaluate them on the holdout data or test data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Make Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the test data\n",
    "latitude_predictions = latitude_predictor.predict(test_data)\n",
    "longitude_predictions = longitude_predictor.predict(test_data)\n",
    "\n",
    "# Create the output DataFrame with predictions\n",
    "output = pd.DataFrame({\n",
    "    'ID': test_data['ID'],  # Row ID from the test set\n",
    "    'longitude_predicted': longitude_predictions,  # Longitude prediction\n",
    "    'latitude_predicted': latitude_predictions     # Latitude prediction\n",
    "})\n",
    "\n",
    "# Display the first few rows of the output DataFrame\n",
    "print(output.head())\n",
    "\n",
    "# Save the output to a CSV file\n",
    "output.to_csv('ais_predictions.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
