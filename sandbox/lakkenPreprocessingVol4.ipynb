{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "pd.set_option('display.max_columns', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../first_50000_rows.csv', sep='|')\n",
    "train = train.drop(['portId','etaRaw'], axis=1)\n",
    "train['time'] = pd.to_datetime(train['time'])\n",
    "train = train.sort_values(by=['vesselId','time'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train[\"vesselId\"].value_counts())\n",
    "\n",
    "# modify train so it only contains vesselId: 6323f2287abc89c0a9631e57 and 61e9f466b937134a3c4c0273\n",
    "\n",
    "#train = train[train['vesselId'].isin(['6323f2287abc89c0a9631e57', '61e9f466b937134a3c4c0273'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vessels = pd.read_csv('../vessels.csv', sep='|')\n",
    "display(vessels.head())\n",
    "\n",
    "# Keep only the vessels that are in the train set\n",
    "print(vessels.shape)\n",
    "vessels = vessels[vessels['vesselId'].isin(train['vesselId'])]\n",
    "print(vessels.shape)\n",
    "print(vessels.describe())\n",
    "\n",
    "# print number of nan values in each column\n",
    "print(vessels.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vessels['shippingLineId'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge vessels into train\n",
    "train = pd.merge(train, vessels, on='vesselId', how='left')\n",
    "train.drop(['NT','depth', 'draft', 'freshWater', 'fuel', 'homePort', 'maxHeight','maxSpeed', 'maxWidth', 'rampCapacity','shippingLineId'], axis=1, inplace=True)\n",
    "train.fillna(-1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_data(df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = df['vesselId'].unique()\n",
    "    for vessel in tqdm(vessel_list):\n",
    "        vessel_data = df[df['vesselId'] == vessel].sort_values(by='time').reset_index(drop=True)\n",
    "        num_rows = vessel_data.shape[0]\n",
    "        if num_rows <= N_KEEP_PAST:\n",
    "            continue  # Skip vessels with insufficient data\n",
    "        for i in range(N_KEEP_PAST, num_rows):\n",
    "            # Collect past N_KEEP_PAST locations and timestamps\n",
    "            past_data = vessel_data.loc[i - N_KEEP_PAST:i - 1].reset_index(drop=True)\n",
    "            current_data = vessel_data.loc[i]\n",
    "            # Prepare a dictionary to hold the features and target\n",
    "            data_row = {}\n",
    "            target_time = current_data['time']\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                #######\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ######\n",
    "            # Add current location as the target\n",
    "            data_row['target_lat'] = current_data['latitude']\n",
    "            data_row['target_lon'] = current_data['longitude']\n",
    "            #data_row['CEU'] = current_data['CEU']\n",
    "            #data_row['DWT'] = current_data['DWT']\n",
    "            #data_row['GT'] = current_data['GT']\n",
    "            #data_row['vesselType'] = current_data['vesselType']\n",
    "            #data_row['breadth'] = current_data['breadth']\n",
    "            #data_row['enginePower'] = current_data['enginePower']\n",
    "            #data_row['length'] = current_data['length']\n",
    "            #data_row['yearBuilt'] = current_data['yearBuilt']\n",
    "            \n",
    "            data_row['vesselId'] = vessel  # Include vesselId if needed\n",
    "            data_row['target_time'] = target_time # Remove eventually\n",
    "            \n",
    "            \n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "    # Create final DataFrame from the list of data rows\n",
    "    final_df = pd.DataFrame(data_rows)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_KEEP_PAST = 5\n",
    "final_train = create_training_data(train, N_KEEP_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_train.shape)\n",
    "display(final_train.tail())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modify Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('../ais_test.csv')\n",
    "test['time'] = pd.to_datetime(test['time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.shape)\n",
    "# modify test so it only contains vesselId: 6323f2287abc89c0a9631e57 and 61e9f466b937134a3c4c0273\n",
    "#test = test[test['vesselId'].isin(['6323f2287abc89c0a9631e57', '61e9f466b937134a3c4c0273'])]\n",
    "print(test.shape)\n",
    "display(test.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_features(train_df: pd.DataFrame, test_df: pd.DataFrame, N_KEEP_PAST: int) -> pd.DataFrame:\n",
    "    data_rows = []  # List to collect all the data rows\n",
    "    vessel_list = test_df['vesselId'].unique()\n",
    "    \n",
    "    for vessel in tqdm(vessel_list):\n",
    "        # Get the test data for this vessel\n",
    "        test_vessel_data = test_df[test_df['vesselId'] == vessel]\n",
    "        for idx, test_row in test_vessel_data.iterrows():\n",
    "            target_time = test_row['time']\n",
    "            ID = test_row['ID']\n",
    "            scaling_factor = test_row['scaling_factor']  # If needed later\n",
    "            \n",
    "            # Get past data from train for this vessel before the target time\n",
    "            vessel_train_data = train_df[(train_df['vesselId'] == vessel) & (train_df['time'] < target_time)]\n",
    "            vessel_train_data = vessel_train_data.sort_values(by='time').reset_index(drop=True)\n",
    "            num_past_points = vessel_train_data.shape[0]\n",
    "            \n",
    "            if num_past_points < N_KEEP_PAST:\n",
    "                # Not enough past data; decide how to handle (skip or pad with NaNs)\n",
    "                continue  # Or handle as per your requirement\n",
    "            \n",
    "            # Get the last N_KEEP_PAST records\n",
    "            past_data = vessel_train_data.iloc[-N_KEEP_PAST:].reset_index(drop=True)\n",
    "            \n",
    "            # Prepare a dictionary to hold the features\n",
    "            data_row = {}\n",
    "            for j in range(N_KEEP_PAST):\n",
    "                past_time = past_data.loc[j, 'time']\n",
    "                # Calculate the difference in minutes between past time and target time\n",
    "                time_diff = (target_time - past_time).total_seconds() / 60.0  # Difference in minutes\n",
    "                data_row[f'minutes_from_target_{j}'] = time_diff\n",
    "                data_row[f'lat_{j}'] = past_data.loc[j, 'latitude']\n",
    "                data_row[f'lon_{j}'] = past_data.loc[j, 'longitude']\n",
    "                ###\n",
    "                data_row[f'cog_{j}'] = past_data.loc[j, 'cog']\n",
    "                data_row[f'sog_{j}'] = past_data.loc[j, 'sog']\n",
    "                data_row[f'rot_{j}'] = past_data.loc[j, 'rot']\n",
    "                data_row[f'heading_{j}'] = past_data.loc[j, 'heading']\n",
    "                data_row[f'navstat_{j}'] = past_data.loc[j, 'navstat']\n",
    "                ###\n",
    "                \n",
    "                #if j == N_KEEP_PAST - 1:\n",
    "                #    data_row['CEU'] = past_data.loc[j, 'CEU']\n",
    "                #    data_row['DWT'] = past_data.loc[j, 'DWT']\n",
    "                #    data_row['GT'] = past_data.loc[j, 'GT']\n",
    "                #    data_row['vesselType'] = past_data.loc[j, 'vesselType']\n",
    "                #    data_row['breadth'] = past_data.loc[j, 'breadth']\n",
    "                #    data_row['enginePower'] = past_data.loc[j, 'enginePower']\n",
    "                #    data_row['length'] = past_data.loc[j, 'length']\n",
    "                #    data_row['yearBuilt'] = past_data.loc[j, 'yearBuilt']\n",
    "            \n",
    "            \n",
    "            # Include vesselId and ID for result matching\n",
    "            data_row['vesselId'] = vessel\n",
    "            data_row['ID'] = ID\n",
    "            # Append the row to the list\n",
    "            data_rows.append(data_row)\n",
    "    \n",
    "    # Create test features DataFrame from the list of data rows\n",
    "    test_features = pd.DataFrame(data_rows)\n",
    "    return test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_final = create_test_features(train, test, N_KEEP_PAST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_final.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_train.to_csv('final_train.csv', index=False)\n",
    "test_final.to_csv('test_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import ElasticNet\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.optimize import minimize\n",
    "import catboost as cb\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "\n",
    "# Load the data\n",
    "test_final = pd.read_csv('test_final.csv')\n",
    "final_train = pd.read_csv('final_train.csv')\n",
    "\n",
    "\n",
    "print(\"Initial shape of training data:\", final_train.shape)\n",
    "final_train = final_train.dropna()\n",
    "print(\"Shape after dropping NaNs:\", final_train.shape)\n",
    "\n",
    "\n",
    "# Define features and targets\n",
    "features = final_train.drop(columns=['target_lat', 'target_lon', 'vesselId', 'target_time'])\n",
    "targets = final_train[['target_lat', 'target_lon']]\n",
    "\n",
    "feature_columns = features.columns\n",
    "X_test = test_final[feature_columns]\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'ridge_optuna': MultiOutputRegressor(Ridge(alpha=1432.7819410601255, random_state=42)),\n",
    "    'lightgbm_optuna': MultiOutputRegressor(lgb.LGBMRegressor(n_estimators=82, learning_rate=0.1000124709018958, num_leaves=256, max_depth=12, min_child_samples=6, subsample=0.7895654034356272, colsample_bytree=0.705074943165495, reg_alpha=0.965623735511052, reg_lambda=0.9415109979963966, verbose=-1, random_state=42, n_jobs=-1)),\n",
    "    'catboost_optuna': MultiOutputRegressor(cb.CatBoostRegressor(iterations=476, learning_rate=0.1596674384872822, depth=10, l2_leaf_reg=3.646475024083615, bagging_temperature=0.12986428988678064, random_state=42, verbose=0, thread_count=-1)),\n",
    "    'histgradientboosting_optuna': MultiOutputRegressor(HistGradientBoostingRegressor(learning_rate=0.11343690296237537, max_iter=456, max_leaf_nodes=157, max_depth=13, min_samples_leaf=31, l2_regularization=0.24506344544473505, max_bins=240, random_state=42, verbose=0)),\n",
    "    'xgboost_optuna': MultiOutputRegressor(xgb.XGBRegressor(n_estimators=422, max_depth=14, learning_rate=0.02124556170140244, subsample=0.614092688023698, colsample_bytree=0.5846203682106957, gamma=0.7660348189459568, reg_alpha=0.5096703756053615, reg_lambda=0.5556686221134866, objective='reg:squarederror', random_state=42, n_jobs=-1)),\n",
    "}\n",
    "\n",
    "# Prepare arrays to hold OOF predictions and test predictions\n",
    "n_splits = 10\n",
    "kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "\n",
    "oof_preds = {model_name: np.zeros((features.shape[0], targets.shape[1])) for model_name in models}\n",
    "test_preds_cv = {model_name: np.zeros((X_test.shape[0], targets.shape[1], n_splits)) for model_name in models}\n",
    "\n",
    "# Perform cross-validation and collect predictions\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(features, targets)):\n",
    "    print(f\"\\nFold {fold + 1}/{n_splits}\")\n",
    "    X_train, y_train = features.iloc[train_idx], targets.iloc[train_idx]\n",
    "    X_valid, y_valid = features.iloc[valid_idx], targets.iloc[valid_idx]\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"Training and predicting with model: {model_name}\")\n",
    "        clf = model\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred_valid = clf.predict(X_valid)\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        \n",
    "        # Save OOF predictions\n",
    "        oof_preds[model_name][valid_idx] = y_pred_valid\n",
    "        # Save test predictions\n",
    "        test_preds_cv[model_name][:, :, fold] = y_pred_test\n",
    "\n",
    "# Define the loss function for optimization\n",
    "def mse_loss(weights):\n",
    "    weights = np.array(weights)\n",
    "    # Normalize weights to sum to 1\n",
    "    weights = weights / np.sum(weights)\n",
    "    # Combine OOF predictions using the weights\n",
    "    final_oof = np.zeros_like(targets.values)\n",
    "    for i, model_name in enumerate(models):\n",
    "        final_oof += weights[i] * oof_preds[model_name]\n",
    "    # Compute mean squared error\n",
    "    mse = mean_squared_error(targets.values, final_oof)\n",
    "    return mse\n",
    "\n",
    "# Optimization methods to try\n",
    "methods = [\n",
    "    'Nelder-Mead', 'Powell', 'CG', 'BFGS',\n",
    "    'L-BFGS-B', 'TNC', 'SLSQP',\n",
    "]\n",
    "\n",
    "# Initial weights\n",
    "initial_weights = np.ones(len(models)) / len(models)\n",
    "\n",
    "# Constraints and bounds\n",
    "constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}\n",
    "bounds = [(0, 1)] * len(models)\n",
    "\n",
    "# Optimize weights using different methods\n",
    "best_mse = np.inf\n",
    "best_weights = None\n",
    "best_method = None\n",
    "\n",
    "for method in methods:\n",
    "    print(f\"\\nOptimizing weights using method: {method}\")\n",
    "    try:\n",
    "        if method in ['SLSQP']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds, constraints=constraints)\n",
    "        elif method in ['L-BFGS-B', 'TNC']:\n",
    "            res = minimize(mse_loss, initial_weights, method=method, bounds=bounds)\n",
    "        else:\n",
    "            # For unconstrained methods, weights will be normalized in mse_loss\n",
    "            res = minimize(mse_loss, initial_weights, method=method)\n",
    "        if res.fun < best_mse:\n",
    "            best_mse = res.fun\n",
    "            best_weights = res.x / np.sum(res.x)  # Normalize weights\n",
    "            best_method = method\n",
    "        print(f\"Method: {method}, MSE: {res.fun}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Method: {method}, failed with error: {e}\")\n",
    "\n",
    "# Average test predictions over folds for each model\n",
    "test_preds_avg = {}\n",
    "for model_name in models:\n",
    "    test_preds_avg[model_name] = np.mean(test_preds_cv[model_name], axis=2)  # Average over folds\n",
    "\n",
    "# Retrain each model on the full training data and predict on the test set\n",
    "print(\"\\nRetraining models on the full training data and generating final predictions.\")\n",
    "final_test_preds = {}\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Retraining model: {model_name}\")\n",
    "    clf = model\n",
    "    clf.fit(features, targets)\n",
    "    y_pred_test = clf.predict(X_test)\n",
    "    final_test_preds[model_name] = y_pred_test\n",
    "\n",
    "# Combine the test predictions using the best weights\n",
    "final_test_pred = np.zeros((X_test.shape[0], targets.shape[1]))\n",
    "for i, model_name in enumerate(models):\n",
    "    final_test_pred += best_weights[i] * final_test_preds[model_name]\n",
    "\n",
    "# Create a DataFrame with IDs and predictions\n",
    "prediction_df = pd.DataFrame({\n",
    "    'ID': test_final['ID'],\n",
    "    'longitude_predicted': final_test_pred[:, 1],\n",
    "    'latitude_predicted': final_test_pred[:, 0]\n",
    "})\n",
    "\n",
    "# Save the predictions to a CSV file\n",
    "prediction_df.to_csv('predictions_ensemble_final.csv', index=False)\n",
    "print(\"\\nPredictions saved to 'predictions_ensemble_final.csv'.\")\n",
    "\n",
    "# Print the best method and weights\n",
    "print(f\"\\nBest optimization method: {best_method}\")\n",
    "print(\"Best weights:\")\n",
    "for i, model_name in enumerate(models):\n",
    "    print(f\"{model_name}: {best_weights[i]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.9772604016895365"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
